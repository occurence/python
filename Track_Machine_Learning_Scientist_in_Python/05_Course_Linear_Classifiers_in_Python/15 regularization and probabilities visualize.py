import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LogisticRegression

X = np.array(pd.read_csv(r'D:\STUDY\python\Track_Machine_Learning_Scientist_in_Python\05_Course_Linear_Classifiers_in_Python\datasets\visual_X.csv'))
y = np.array([0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,5,4,8,8,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,3,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,2,5,7,9,5,4,4,9,0,8,9,8,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,9,5,5,6,5,0,9,8,9,8,4,1,7,7,3,5,1,0,0,2,2,7,8,2,0,1,2,6,3,3,7,3,3,4,6,6,6,4,9,1,5,0,9,5,2,8,2,0,0,1,7,6,3,2,1,7,4,6,3,1,3,9,1,7,6,8,4,3,1,4,0,5,3,6,9,6,1,7,5,4,4,7,2,8,2,2,5,7,9,5,4,8,8,4,9,0,8,9,8,])

def show_digit(i, lr=None):
    plt.imshow(np.reshape(X[i], (8,8)), cmap='gray', vmin = 0, vmax = 16, interpolation=None)
    plt.xticks(())
    plt.yticks(())
    if lr is None:
        plt.title("class label = %d" % y[i])
    else:
        pred = lr.predict(X[i][None])
        pred_prob = lr.predict_proba(X[i][None])[0,pred]
        # plt.title("label=%d, prediction=%d, proba=%.2f" % (y[i], pred, pred_prob))
        plt.title(f"label={y[i]}, prediction={pred[0]}, proba={pred_prob[0]:.2f}")

    plt.show()

lr = LogisticRegression(solver='liblinear')
lr.fit(X,y)

# Get predicted probabilities
proba = lr.predict_proba(X)

# Sort the example indices by their maximum probability
proba_inds = np.argsort(np.max(proba,axis=1))

# Show the most confident (least ambiguous) digit
show_digit(proba_inds[-1], lr)

# Show the least confident (most ambiguous) digit
show_digit(proba_inds[0], lr)

print("Most confident index:", proba_inds[-1])
print("Least confident index:", proba_inds[0])

print(X[:5])  # First 5 feature rows
print(y[:5])  # First 5 labels

pred = lr.predict(X)
print(pred[:10])

print("Local Min/Max:", X.min(), X.max())

print(proba[1658])
print(proba[1660])

print(proba_inds)

for elem in proba_inds:
    print(f"{elem}")

# with open("proba_inds.txt", "w") as f:
#     for elem in proba_inds:
#         f.write(f"{elem}\n")

print(proba)